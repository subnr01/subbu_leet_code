

===================================================================================================================
Given a set of 1 Trillion integers on hard disk, find the smallest 1 million of them. You can fit at most 1 million integers in memory at a time.
===================================================================================================================

You can do this efficiently in O(n log m) by using a heap. ( n = all the numbers, m = the size of the set of numbers you want to find ).

Go through the trillion numbers one at a time. For each new number do one of the following.

If the heap has < 1 million nodes insert the new number into the heap.
If the heap has exactly 1 million nodes and the top node is > than the new number, then pop the top node from the heap, and insert a node with the new number.
If neither 1 or 2 are true then toss the number.
After you go through all the trillion entries then the resulting heap will have the 1 million smallest numbers.

Inserting and deleting from the heap is O(log m). The single pass through the heap is n. So, the algorithm is n*log (m)




===================================================================================================================
Write a program to find 100 largest numbers out of an array of 1 billion numbers. </stackoverflow>
===================================================================================================================
up vote
315
down vote
accepted
You can keep a priority queue of the 100 biggest numbers, iterate through the billion numbers, whenever you encounter a number greater than the smallest number in the queue (the head of the queue), remove the head of the queue and add the new number to the queue.

EDIT: as Dev noted, with a priority queue implemented with a heap, the complexity of insertion to queue is O(logN)

In the worst case you get billionlog2(100) which is better than billionlog2(billion)

In general, if you need the largest K numbers from a set of N numbers, the complexity is O(NlogK) rather than O(NlogN), this can be very significant when K is very small comparing to N.

EDIT2:

The expected time of this algorithm is pretty interesting, since in each iteration an insertion may or may not occur. The probability of the i'th number to be inserted to the queue is the probability of a random variable being larger than at least i-K random variables from the same distribution (the first k numbers are automatically added to the queue). We can use order statistics (see link) to calculate this probability. For example, lets assume the numbers were randomly selected uniformly from {0, 1}, the expected value of (i-K)th number (out of i numbers) is (i-k)/i, and chance of a random variable being larger than this value is 1-[(i-k)/i] = k/i.

(k time to generate the queue with the first k elements, then n-k comparisons, and the expected number of insertions as described above, each takes an average log(k)/2 time)

Note that when N is very large comparing to K, this expression is a lot closer to n rather than NlogK. This is somewhat intuitive, as in the case of the question, even after 10000 iterations (which is very small comparing to a billion), the chance of a number to be inserted to the queue is very small.
